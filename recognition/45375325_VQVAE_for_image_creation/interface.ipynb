{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_vqvae, DEVICE\n",
    "from modules.vqvae import VQVAE\n",
    "from dataset import train_dl, test_dl, NumpyDataset, codebook_transform, batch_size\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show(img):\n",
    "    \"\"\"\n",
    "    Plotting func\n",
    "    \"\"\"\n",
    "    np_img = img.numpy()\n",
    "    fig = plt.imshow(np.transpose(np_img, (1, 2, 0)), interpolation='nearest')\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAE(latent_dim=128, res_h_dim=32, num_embeddings=512, embedding_dim=64, beta=0.25)\n",
    "model.to(DEVICE)\n",
    "EPOCHS = 2\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================EPOCH = 1======================\n",
      "batch    0/354 \t |current loss: 1.367112\n",
      "batch   25/354 \t |current loss: 0.695352\n",
      "batch   50/354 \t |current loss: 0.356504\n",
      "batch   75/354 \t |current loss: 0.252510\n",
      "batch  100/354 \t |current loss: 0.244847\n",
      "batch  125/354 \t |current loss: 0.157223\n",
      "batch  150/354 \t |current loss: 0.143243\n",
      "batch  175/354 \t |current loss: 0.166342\n",
      "batch  200/354 \t |current loss: 0.186239\n",
      "batch  225/354 \t |current loss: 0.135978\n",
      "batch  250/354 \t |current loss: 0.263740\n",
      "batch  275/354 \t |current loss: 0.202205\n",
      "batch  300/354 \t |current loss: 0.156482\n",
      "batch  325/354 \t |current loss: 0.116243\n",
      "batch  350/354 \t |current loss: 0.127772\n",
      "Reconstruction loss: 0.2808905892958075\n",
      "=======================EPOCH = 2======================\n",
      "batch    0/354 \t |current loss: 0.099904\n",
      "batch   25/354 \t |current loss: 0.140721\n",
      "batch   50/354 \t |current loss: 0.216068\n",
      "batch   75/354 \t |current loss: 0.151931\n",
      "batch  100/354 \t |current loss: 0.213867\n",
      "batch  125/354 \t |current loss: 0.207343\n"
     ]
    }
   ],
   "source": [
    "training_reconstruction_loss = []\n",
    "for i in range(EPOCHS):\n",
    "    print(f\"=======================EPOCH = {i + 1}======================\")\n",
    "    loss = train_vqvae(dl=train_dl, model=model, optim=optim)\n",
    "    training_reconstruction_loss.append(loss)\n",
    "    print(f\"Reconstruction loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real = next(iter(test_dl))  # load some from test dl\n",
    "test_real = test_real[0]\n",
    "test_real = test_real.to(DEVICE)\n",
    "pre_conv = model.pre_quantization_conv(model.encoder(test_real))  # encoder, reshape\n",
    "_, test_quantized, _, _ = model.vector_quantizer(pre_conv)\n",
    "test_reconstructions = model.decoder(test_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show reconstructed images\n",
    "show(torchvision.utils.make_grid(test_reconstructions.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show original images\n",
    "show(torchvision.utils.make_grid(test_real.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = next(iter(test_dl))\n",
    "test_input = test_input[0][0]\n",
    "test_input = test_input.unsqueeze(0)\n",
    "print(test_input.shape)\n",
    "test_input = test_input.to(DEVICE)\n",
    "test_encoded = model.encoder(test_input)\n",
    "test_encoded = model.pre_quantization_conv(test_encoded)\n",
    "_, test_encoded, encodings, indices = model.vector_quantizer(test_encoded)\n",
    "decoded = model.decoder(test_encoded)\n",
    "# z is codebook index\n",
    "# Plot codebook index\n",
    "plot_image = indices.view(64, 64)\n",
    "print(torch.unique(indices.to('cpu')))\n",
    "plot_image = plot_image.to('cpu')\n",
    "detached_image = plot_image.detach().numpy()\n",
    "\n",
    "test_input = test_input[0][0].cpu().detach().numpy()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Real vs Codebook index')\n",
    "ax1.imshow(test_input)\n",
    "ax2.imshow(detached_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Codebook indice to quatized\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.suptitle('Codebook indice vs quantized')\n",
    "zz = model.vector_quantizer.get_quantized(indices)\n",
    "zzz = model.decoder(zz)\n",
    "immi = zzz[0]\n",
    "immi = immi.to('cpu')\n",
    "immi = immi.detach().numpy()\n",
    "ax2.imshow(immi[1])\n",
    "ax1.imshow(indices.cpu().view(64, 64).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = next(iter(test_dl))\n",
    "codebook_data = []\n",
    "while True:\n",
    "    next_val = next(iter(test_dl),'end')\n",
    "    if next_val == 'end':\n",
    "        break\n",
    "    else:\n",
    "        current_input = next_val[0][0]\n",
    "        current_input = current_input.unsqueeze(0)\n",
    "        current_input = current_input.to(DEVICE)\n",
    "        encoded = model.encoder(current_input)\n",
    "        encoded = model.pre_quantization_conv(encoded)\n",
    "        _, encoded, encodings, indices = model.vector_quantizer(encoded)\n",
    "        decoded = model.decoder(encoded)\n",
    "        # z is codebook index\n",
    "        # Plot codebook index\n",
    "        plot_image = indices.view(64, 64)\n",
    "        print(torch.unique(indices.to('cpu')))\n",
    "        plot_image = plot_image.to('cpu')\n",
    "        detached_image = plot_image.detach().numpy()\n",
    "\n",
    "        codebook_data.append(detached_image)\n",
    "\n",
    "\n",
    "codebook_set = NumpyDataset(data=codebook_data, targets=[1], transform=codebook_transform)\n",
    "codebook_loader = DataLoader(codebook_set, batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
